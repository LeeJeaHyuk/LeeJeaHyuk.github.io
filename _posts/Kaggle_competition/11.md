# 코드 이해

[AI4Code Pytorch DistilBert Baseline](https://www.kaggle.com/code/aerdem4/ai4code-pytorch-distilbert-baseline/notebook) 코드 한 줄씩 이해해보기

### 1

```python
import json
from pathlib import Path

import numpy as np
import pandas as pd
from scipy import sparse
from tqdm import tqdm

pd.options.display.width = 180
pd.options.display.max_colwidth = 120
# 옵션 바꾸기

BERT_PATH = "../input/huggingface-bert-variants/distilbert-base-uncased/distilbert-base-uncased"

data_dir = Path('../input/AI4Code')
```

##### pathlib

모듈의 기본 아이디어는 파일시스템 경로를 단순한 문자열이 아니라 객체로 다루자는 것

##### from scipy import sparse

sparse matrix(희소행렬) : 행렬의 원소의 값이 대부분 0인 행렬

dense matrix(밀집행렬) : 행렬의 원소의 값이 대부분 0이 아닌 행렬\



###### TF-IDF(Term Frequency-Inverse Document Frequency) : 단어마다 중요도를 고려하여 가중치를 두는 통계적 수치

TF(단어 빈도, term frequency) :  특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값으로, 이 값이 높을수록 문서에서 중요하다고 생각할 수 있다

DF(문서 빈도, document frequency) : 단어 자체가 문서군 내에서 자주 사용되는 경우, 이것은 그 단어가 흔하게 등장한다는 것을 의미한다.

IDF(역문서 빈도, inverse document frequency) : TF값의 역수 

TF-IDF : TF와 IDF를 곱한 값이다



TF-IDF 행렬을 만들 때 sparse matrix가 자주 발생하는데 메모리 낭비와 연산시간을 줄이기 위해  자료구조를 바꾸어주는 경우에 사용하는 것으로 보인다.

#### from tqdm import tqdm

진행상황을 보기 위한 바



### 2

```python
NUM_TRAIN = 10000 
# train set 개수

def read_notebook(path):
    return (
        pd.read_json(
            path,
            dtype={'cell_type': 'category', 'source': 'str'})
        .assign(id=path.stem)
        .rename_axis('cell_id')
    )


paths_train = list((data_dir / 'train').glob('*.json'))[:NUM_TRAIN]
notebooks_train = [
    read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')
]

df = (
    pd.concat(notebooks_train)
    .set_index('id', append=True)
    .swaplevel()
    .sort_index(level='id', sort_remaining=False)
)

df
```

assign으로 새 열을 할당

read_notebook(path) for path in tqdm(paths_train, desc='Train NBs')

![image-20220707110547861](../../images/11/image-20220707110547861.png)

swaplevel() 두 인덱스의 순서를 바꾸는 메서드

sort_remaining: multi index의 경우 다른 레벨에 대해서도 정렬을할지 여부입니다. `True`로 할 경우 한 레벨에 대한 정렬이 완료되면, 다른 레벨도 정렬합니다



glob('*.json') 모든 json파일 찾기



```python
df_orders = pd.read_csv(
    data_dir / 'train_orders.csv',
    index_col='id',
    squeeze=True,
).str.split()  # Split the string representation of cell_ids into a list

df_orders
```

![image-20220707173022229](../../images/11/image-20220707173022229.png)

squeeze=True 를 사용하면 가능한 한 차수를 줄이려고 하는 것

.str.split() 문자열을 리스트로?

문자열을 리스트형태로 저장

셀을 순서대로 리스트로 저장





![image-20220707173116205](../../images/11/image-20220707173116205.png)

![image-20220707161306042](../../images/11/image-20220707161306042.png)



```
# Get the correct order
cell_order = df_orders.loc[nb_id]

print(f"cell_order \n\n{cell_order} \n")
print("The ordered notebook:")

nb.loc[cell_order, :]
```

df_orders.loc[nb_id] : id가 index 역할을 해서 nb_id에 해당하는 id의 ids_index를 가져옵니다.

[nb_id]에 접근해서 열을 모두 가져와서 cell_order로 넘겨준다.



##### loc[row,column]

첫 번째에는 인덱싱할 row를 넣고, 두 번째에는 인덱싱할 column을 넣으면 된다. row나 column의 이름을 넣어도 되고, 리스트를 넣어도 되며, 리스트 슬라이싱을 넣어도 원하는 결과가 나온다.



![image-20220707174802190](../../images/11/image-20220707174802190.png)

![image-20220707174652566](../../images/11/image-20220707174652566.png)



```
def get_ranks(base, derived):
    return [base.index(d) for d in derived]

cell_ranks = get_ranks(cell_order, list(nb.index))
nb.insert(0, 'rank', cell_ranks)
```

return [base.index(d) for d in derived] d의 위치 반환

rank(위치)를 insert로 삽입



![image-20220707194949818](../../images/11/image-20220707194949818.png)

![image-20220707194933799](../../images/11/image-20220707194933799.png)





```
df_orders_ = df_orders.to_frame().join(
    df.reset_index('cell_id').groupby('id')['cell_id'].apply(list),
    how='right',
)

ranks = {}
for id_, cell_order, cell_id in df_orders_.itertuples():
    ranks[id_] = {'cell_id': cell_id, 'rank': get_ranks(cell_order, cell_id)}

df_ranks = (
    pd.DataFrame
    .from_dict(ranks, orient='index')
    .rename_axis('id')
    .apply(pd.Series.explode)
    .set_index('cell_id', append=True)
)
```

![image-20220707201058041](../../images/11/image-20220707201058041.png)

![image-20220707201255561](../../images/11/image-20220707201255561.png)

![image-20220707201319394](../../images/11/image-20220707201319394.png)

cell_id 가 set_index를 통해서 index로 변환

.apply(pd.Series.explode) 로 series를 행으로 변환하는 것 같음

reset_index는 index를 제설성하는 것

 

```
df_ancestors = pd.read_csv(data_dir / 'train_ancestors.csv', index_col='id')
df_ancestors
```

![image-20220707202607763](../../images/11/image-20220707202607763.png)

fork 되었는지를 확인하기 위해서 data_dir/'train_ancestors.csv'  불러오기

**index_col** : index이름정하기



```
df = df.reset_index().merge(df_ranks, on=["id", "cell_id"]).merge(df_ancestors, on=["id"])
df
```

![image-20220707202936162](../../images/11/image-20220707202936162.png)

df에 df_ancestors merge해서 열 추가한 것으로 보임



```
df["pct_rank"] = df["rank"] / df.groupby("id")["cell_id"].transform("count")

df["pct_rank"].hist(bins=10)
```

![image-20220707203258157](../../images/11/image-20220707203258157.png)![image-20220707203322211](../../images/11/image-20220707203322211.png)

rank : 각 코드의 순서?



```
from sklearn.model_selection import GroupShuffleSplit

NVALID = 0.1  # size of validation set

splitter = GroupShuffleSplit(n_splits=1, test_size=NVALID, random_state=0)

train_ind, val_ind = next(splitter.split(df, groups=df["ancestor_id"]))

train_df = df.loc[train_ind].reset_index(drop=True)
val_df = df.loc[val_ind].reset_index(drop=True)
```

GroupShuffleSplit로 데이터를 잘 섞는다는 것 같음

![image-20220707203907674](../../images/11/image-20220707203907674.png)



```
from bisect import bisect

def count_inversions(a):
    inversions = 0
    sorted_so_far = []
    for i, u in enumerate(a):
        j = bisect(sorted_so_far, u)
        inversions += i - j
        sorted_so_far.insert(j, u)
    return inversions

def kendall_tau(ground_truth, predictions):
    total_inversions = 0
    total_2max = 0  # twice the maximum possible inversions across all instances
    for gt, pred in zip(ground_truth, predictions):
        ranks = [gt.index(x) for x in pred]  # rank predicted order in terms of ground truth
        total_inversions += count_inversions(ranks)
        n = len(gt)
        total_2max += n * (n - 1)
    return 1 - 4 * total_inversions / total_2max
```





# 참고

[sparse](https://rfriend.tistory.com/551)

[TF-IDF](https://ko.wikipedia.org/wiki/Tf-idf)

[.apply(pd.Series.explode)](https://www.w3resource.com/pandas/series/series-explode.php)

