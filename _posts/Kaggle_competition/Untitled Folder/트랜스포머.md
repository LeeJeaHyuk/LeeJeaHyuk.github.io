# Transformer

[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)

기존 인코더 / 디코더를 발전시키면서 RNN을 사용하지 않는 모델

학습 속도가 빠르면서 좋은 결과를 냄



Parallelization

병렬화 (일을 최대한 한번에 처리함)



RNN based encoder decoder with attention

고정된 문맥 벡터를 사용하지 않고 인코더에서 각 단어의 상태값을 계산한다

동적으로 인코더를 사용하기 때문에 긴 문장의 번역의 성능을 강화



attention만으로도 입력 데이터에서 중요한 정보들을 찾아내기



### attention

디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 점입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 됩니다.



