# 로지스틱 회귀

회귀이지만 분류 모델에서 사용한다.  왜 그런지에 대해서 검색해봤더니 위키 백과에서 그 이유를 어느정도 추측할 수 있었다.

로지스틱 회귀분석에서는 종속 변수가 이항분포를 따르는 모형을 사용하기 때문에 타겟 데이터를 범주형 데이터로 사용해야 하므로 로지스틱 회귀이지만 분류의 역할을 하게 되는것이라고 생각된다. 

그렇다면 왜 로지스틱 분류가 아닌 회귀인가?

로지스틱 회귀는 선형 회귀처럼 독립 변수의 선형 결합으로 종속 변수를 설명하기 때문이다.



그러면 로지스틱 함수는 어떻게 정의되었는지 살펴보자

## logistic(sigmoid) function

베르누이 시행에서 1이 나올 확률을 p 0이 나올 확률과 1-p라고 했을 때 이것의 비율을
$$
odds=\frac{p}{1-p}
$$
라고 하고 이것에 로그를 취하면 logit함수가 된다.
$$
z=logit(odds)=log(\frac{p}{1-p})
$$
![image-20220504000128498](../../images/2022-05-02 로지스틱 회귀(혼공머신4-1)/image-20220504000128498.png)

이것의 역함수를 취해주면 로지스틱(시그모이드)함수가 된다
$$
p=log(\frac{z}{1-z})
\\
e^p=\frac{z}{1-z}
\\
p=\frac{e^z}{1+e^z}
\\
p=\frac{1}{1-e^{-z}}
\\
\phi=\frac{1}{1+e^{-z}}
$$
![KakaoTalk_20220502_212813819](../../images/2022-05-02 로지스틱 회귀(혼공머신4-1)/KakaoTalk_20220502_212813819.jpg)	

## 로지스틱 회귀

[Fish Market](https://www.kaggle.com/aungpyaeap/fish-market) 데이터를 사용해서 Species 제외한 다른 특성을 독립변수로  Species 중에서 Bream,Smelt를 타겟으로 하는 로지스틱 회귀 분석을 해 보자

```python
y=df['Species']
X=df.drop(['Species'],axis=1)

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(X, y, random_state=42)
```

데이터를 준비해 주고

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]
```

StandardScaler를 통해서 표준화를 한번 해 준 다음 Species중 Bream,Smelt 만 따로 뽑아준다.

```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
```

```python
print(lr.predict_proba(train_bream_smelt[:5]))
```

LogisticRegression으로 훈련하고 테스트 5개 샘플의 predict확률을 보면 음성 클래스와 양성 클래스의 확률을 출력해준다.

![image-20220504073233557](../../images/2022-05-02 로지스틱 회귀(혼공머신4-1)/image-20220504073233557.png)

```python
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)
```

![image-20220504072816825](../../images/2022-05-02 로지스틱 회귀(혼공머신4-1)/image-20220504072816825.png)

LogisticRegression은 또한 decision값(z)값을 확인할 수있는데 이 값을 로지스틱 함수에 넣어 주면 확률값을 구할 수 있게 될 것이다.

```python
from scipy.special import expit

print(expit(decisions))
```

 SciPy 라이브러리에서 시그모이드함수를 가져와서 값을 넣어주면 양성클래스의 확률과 동일한 값을 출력한다.

![image-20220504073450977](../../images/2022-05-02 로지스틱 회귀(혼공머신4-1)/image-20220504073450977.png)

# 로지스틱 회귀를 사용한 다중 분류 

LogisticRegression이 로지스틱 함수를 사용한 회귀를 한다는 것을 확인했으니 이제 Species 를 전부 사용해서 다중 분류를 해 보자 

```
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

로지스틱 회귀도 릿지 회귀처럼 L2규제를 사용하는데 위의 하이퍼파리미터 C는 릿지 회귀에서의 람다의 역수이다. 그러므로 값이 작을수록 규제가 강화된다.







 

## 참고

HOWARD ANTON, CHRIS RORRES, 알기쉬운 선형대수 개정 11판, 김태균,박미경,이호재,이희정,정정주,한광희 옮김, (주)한티에듀, 2019년, 427~430쪽



https://ko.wikipedia.org/wiki/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80



박해선,혼자 공부하는 머신러닝, 한빛미디어, 2021,183~196p