# 인공 신경망

### 패션 MNIST

karas에 내장되어있는 패션 MNIST 데이터를 사용

10개의 클래스와 6만개의 $ 28\times 28 $ 흑백 데이터 샘플로 이루어져 있다.



## 로지스틱 회귀

```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
# 훈련과 테스트 데이터의 입력과 타겟쌍을 반환하기

print(train_input.shape, train_target.shape)
# (60000, 28, 28) (60000,) 28x28 이미지 데이터가 6만개 존재

print(test_input.shape, test_target.shape)
# (10000, 28, 28) (10000,) 테스트데이터는 만개 존재
```



### 입력과 타겟 샘플

```python
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')    
    # 값이 높아야 계산하기 편하기 때문에 반전해준 모습(배경이 검은색으로 보이기 때문에)
    axs[i].axis('off')
plt.show()
```





```python
print([train_target[i] for i in range(10)])
#[9, 0, 0, 3, 0, 2, 7, 2, 5, 5] 타겟 데이터 0~9 정수값

import numpy as np

print(np.unique(train_target, return_counts=True))
# return_counts=True로 하면 클래스 정수값이 어떤 게 들어가 있는지, 각각 정수값이 얼마만큼 들어가 있는지 세준다.
(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))
#클래스마다 6000개씩 고르게 들어가있다. (예제 데이터이므로)
```



## 로지스틱 회귀로 패션 아이템 분류하기

```python
train_scaled = train_input / 255.0 # 픽셀값이 0~255이므로 255로 나누어주면 0~1로 표준화되는 효과가 있다.
train_scaled = train_scaled.reshape(-1, 28*28) #차원을 곲해서 하나의 차원으로 만들어주기위해서
print(train_scaled.shape) # 28x28 이미지를 펼쳐서 하나의 차원으로
#(60000, 784)

from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log', max_iter=5, random_state=42)
# 로지스틱 함수를 바로 쓰지않고 경사하강법(SGD)에서 loss함수를 log로 지정

scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))
# 0.8195666666666668 훈련 데이터의 cross_validate의 교차검증점수의 평균값
# test_score이지만 훈련 데이터로 만든 검증 데이터이다.
```



#### 로지스틱 손실함수

10개의 클래스를 다중 분류를 하는데 SGD에 크로스엔트로피 손실함수를 지정할수는 없기 때문에 로지스틱 손실함수를 지정하는데 로지스틱 손실함수는 클래스만큼의 이진분류를 시행한 후에 소프트맥스를 사용하는 식으로 해결한다.

OVA

OVR



## 인공 신경망과 로지스틱 회귀

출력층이 한개만 존재하는 가장 간단한 인공 신경망은 경사하강법을 사용한 로지스틱 회귀나 선형 회귀 모델과 구분할 수 없다

출력층 마지막 클래스들의 확률을 출력하는 층

입력층 입력데이터가 놓여있는 층

유닛,뉴런 : 값을 알기를 원하는 클래스들

절편값은 유닛과 뉴런에 항상 더해지는데 생략되는 경우가 많다.



## 케라스 모델

```python
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled.shape, train_target.shape)
# (48000, 784) (48000,)

print(val_scaled.shape, val_target.shape)
# (12000, 784) (12000,)

dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
# Dense 밀집층 fully connected layer
# 10개의 밀집층과 784개의 입력층이 전부 곱해진다.
# 다중 분류이기 때문에 softmax를 사용한다. 이중분류 ='sigmoid'
# 첫번째 층에는 input_shape지정해준다. 크기는 샘플의 크기와 같다.

model = keras.Sequential(dense)
# Sequential로 여러 개의 층을 합치지만 위에 예시에는 1개뿐이다.
# 
```



밀집층 사진

출력층은 항상 유닛의 개수와 같아야 한다.



## 모델 설정

```python
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
# 모델 설정 작업 손실함수를 정해주기
# metrics 매개변수를 지정해서 어떤 측정지표를 알고 싶을때 지정해준다. 위에서는 정확도를 추가한 경우이다.
# compile = 설정이라고 이해하면 된다.
# sparse_categorical_crossentropy는 다중 분류에서 binary-crossentropy는 이진 분류에서
# sparse_ 가 붙은 이유는 다중 분류의 타깃값이 원핫 인코딩이 되어 있지 않은 경우 지정해준다. 원핫인코인 되어 있으면 sparse_없이 사용한다.

print(train_target[:10])
#[7 3 5 8 6 9 3 3 9 9]
```



### sparse_

그림





```python
model.fit(train_scaled, train_target, epochs=5)
# Epoch 1/5
# 1500/1500 [==============================] - 3s 1ms/step - loss: 0.6058 - accuracy: 0.7932
# Epoch 2/5
# 1500/1500 [==============================] - 2s 1ms/step - loss: 0.4785 - accuracy: 0.8385
# Epoch 3/5
# 1500/1500 [==============================] - 2s 1ms/step - loss: 0.4564 - accuracy: 0.8471
# Epoch 4/5
# 1500/1500 [==============================] - 2s 1ms/step - loss: 0.4435 - accuracy: 0.8539
# Epoch 5/5
# 1500/1500 [==============================] - 2s 2ms/step - loss: 0.4358 - accuracy: 0.8551
# <keras.callbacks.History at 0x7f329a2c34c0>
# accuracy 정보도 알려주는 모습을 볼 수 있다.

model.evaluate(val_scaled, val_target) #입력값과 타겟값
#375/375 [==============================] - 1s 1ms/step - loss: 0.4579 - accuracy: 0.8483 
# 85%의 검증 정확도를 얻었다.
#[0.45794257521629333, 0.8483333587646484]
```





### 사이킷런과 케라스 비교

사이킷런은 클래스를 만들 때 가능한 한 많은 매개변수를 지정한다.

```python
```



케라스는 층을 따로 만들고

층을 모델 클래스에 추가하고 

설정을 손실함수나 추가적인 지표를 출력하기 위한 설정들은 compile메소드에서 한다.

fit evaluate는 사이킷런과 비슷한다.

다양하게 모델을 조합할 수 있도록 층 / 모델 / 설정이 따로 있다. 

```python
```



# 참고

