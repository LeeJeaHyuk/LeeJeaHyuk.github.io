# 트리의 앙상블

정형 데이터 : 어떤 구조(CSV,database,excel)에 저장되는 데이터

비정형 데이터 : 데이터베이스나 엑셀로 표현하기 어려운 것들 (텍스트 사진 음악 등)

비정형 데이터에는 신경망 알고리즘을 사용한다. 전통적인 머신러닝 방법으로는 모델을 만들기 까다롭다.



앙상블 : 여려 개의 모델을 사용해서 하나의 결과를 만들어내는 것

앙상블 학습은 정형 데이터를 다루는 데 가장 뛰어난 성과를 내는 알고리즘이다.

앙상블이 트리에 사용하는 이유 :  트리는 성능이 좋지만 과대적합이 쉬우므로 이 트리를 여러개를 만들어서 과대적합을 줄일 수 있으면 좋은 모델이 탄생할 것



## 랜덤 포레스트

![랜덤 포레스트](../../images/2022-05-09-트리의 앙상블(5-3)/랜덤 포레스트.png)

결정 트리를 랜덤하게 만들어서 결정 트리의 숲을 만든다.

그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다.

앙상블에서는 랜덤성을 이용해서 트리의 성능을 낮춘다.



### 부트스트랩 샘플

![부트스트랩](../../images/2022-05-09-트리의 앙상블(5-3)/부트스트랩.png)

랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만든다.

훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다(복원 추출/중복을 허용한 샘플링)

이 샘플을 부트스트랩 샘플이라고 한다. 훈련 세트만큼 뽑기 때문에 훈련 세트와 크기가 같다.

분류의 경우에는 각 부트스트랩 샘플에서 결정 트리 훈련을 하면 나오는 확률들을 모두 더해서 트리 개수로 나누어준다.

회귀의 경우에도 결정 트리 훈련에서 각 예측값이 나오게 되는데 그 값들을 트리개수만큼 나우어서 평균내준다.

![랜덤 포레스트 구조](../../images/2022-05-09-트리의 앙상블(5-3)/랜덤 포레스트 구조.png)

결정트리 훈련을 하여 결정트리 한 개를 만드는 경우에 

노드를 분할할 때 이전에는 불순도차이가 최대가 되도록 만드는데 원래는 모든 특성을 사용하지만

 전체 특성 중에서 일부 특성을  무작위로 고른 다음에 이 중에서 최선의 분할을 찾는다.

RandomForestClassifier는 전체 특성의 제곱근만큼의 특성을 랜덤하게 골라서 선택한다.

 Scikit-Learn의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.

랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용했기 때문에 훈련 세트에 과대적합되는것을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.



코드를 살펴보면

```python
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_jobs=-1, random_state=42)
scores = cross_validate(rf, train_input, train_target,
                        return_train_score=True, n_jobs=-1)
# cross_validate (RandomForestClassifier, 훈련 , 타깃값, 훈련 세트 점수 반환 : True) 

print(np.mean(scores['train_score']), np.mean(scores['test_score']))
# 훈련 세트 점수, 검증 세트 점수
# 0.9973541965122431 0.8905151032797809 

rf.fit(train_input, train_target)
print(rf.feature_importances_)
# 특성의 중요도 확인 
# [0.23167441 0.50039841 0.26792718]
# DecisionTreeClassifier 특성 중요도 [0.12345626 0.86862934 0.0079144 ]

rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)

rf.fit(train_input, train_target)
print(rf.oob_score_)
# oob 샘플을 사용한 스코어
# 0.8934000384837406

```



![image-20220510172921182](../../images/2022-05-09-트리의 앙상블(5-3)/image-20220510172921182.png)

DecisionTreeClassifier의 특성 중요도가 당도가 높게 나온 것에 비해서 RandomForestClassifier랜덤하게 사용하기 때문에 다른 특성들도 강제로 사용하게 되는 부분이 있어서 당도의 특성이 내려가고 다른 특성의 중요도가 올라간 것을 볼 수있다.



부트스트랩은 중복을 허용한 샘플링을 하기 때문에 트리에서 사용하지 않는 부분들 이 생기는데 이 부분을 OOB라고 한다.

중복을 허용해서 램덤하게 샘플을 추출하기때문에 남는 부분도 랜덤하게 결정된다. 그러므로 이 부분을 사용한다면 검증세트의 역할을 할 수 있게 된다.

## 엑스트라 트리

랜덤 포레스트와 매우 비슷하게 동작

랜덤 포레스트와의 차이점은 부트스트랩을 사용하지 않는다는 것

각 결정 트리 모델을 만들 때  전체 훈련 세트를 사용

노드를 분할할 때 가장 좋은 분할을 찾는 것이 아니라 무작위로 분할한다.

splitter=random으로 지정한 결정 트리 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다.



## 그레이디언트 부스팅

GradientBoostingClassifier은 기본적으로 깊이가 3인 결정 트리를 100개 사용

깊이가 얕은 결정 트리를 사용하기 때문에 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.

경사 하강법을 사용하여 트리를 앙상블에 추가



## 히스토그램 기반 그레이디언트 부스팅

정형 데이터를 다루는 머신러닝 알고리즘 주엥 가장 인기가 높은 알고리즘

입력 특성을 256개 구간으로 나누어서 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.



## 앙상블 학습을 통한 성능 향상







