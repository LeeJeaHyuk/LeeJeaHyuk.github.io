# 혼공머신 6-2 K-평균

6-1에서 사용한 데이터는 이미 레이블이 되어있는 데이터이기때문에 클러스터링하는 것이 매우 쉬웠다

하지만 실제 비지도 학습에서는 각 이미지들이 레이블되어있지 않을 것이기 때문에 클러스터링 할 수 있는 방법이 필요할 것이다.



## 클러스터 중심

클러스터 중심 = 센트로이드(centroid)

각 군집(클러스터)의 평균 이미지로서 각 클러스터의 기준이 된다.



## k-평균 알고리즘

1.무작위로 k개(하이퍼 파라미터)의 클러스터 중심을 만들어서 군집의 개수를 정해준다.

2.각 샘플에서 가장 가까운 클러스터 중심을 찾아서 해당 클러스터의 샘플로 지정한다.

3.클러스터에 속한 샘플들의 평균값으로 클러스터 중심을 변경한다.

4.클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다.



## KMeans 클래스

[300개의 과일 데이터](https://bit.ly/fruits_300_data)를 사용



KMeans로 훈련해보자

```python
import numpy as np

fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100) # 이미지 차원 1차원으로 변경

from sklearn.cluster import KMeans

km = KMeans(n_clusters=3, random_state=42) #클러스터 개수 지정
km.fit(fruits_2d)
# train / test를 따로 나누지 않는다.
# 군집에서는 과대적합 괴소적합이라는 개념이 따로 없다.

print(km.labels_)

# [2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
#  2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2
#  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
#  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
#  1 1 1 1]
# n_clusters=3으로 지정했기 때문에 0~2의 값이 지정된것을 볼 수 있다.

print(np.unique(km.labels_, return_counts=True))
#(array([0, 1, 2], dtype=int32), array([111,  98,  91]))
#return_counts를 True로 해주면 unique값 0,1,2를 보여준다.
# unique함수를 통해 개수를 알 수 있다. 0 111개 / 1 98개 / 2 91개 
```

KMeans의 n_iter의 기본값은 10이다 이 중에서 가장 좋은 결과를 출력한다.

결과를 보면  ([111,  98,  91])로 각 클러스터의 개수를 지정했는데

실제 값들은 각각 100개일 것이므로 각 샘플이 클러스터 안에 조금 섞여 있다는 것을 알 수 있다.

