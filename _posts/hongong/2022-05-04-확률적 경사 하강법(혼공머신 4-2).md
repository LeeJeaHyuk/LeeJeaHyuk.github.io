# 확률적 경사 하강법

Gradient Descent

여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘으로 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정한다.

## 비용 함수

비용 함수는 선형 모델의 예측과 훈련 데이터 사이의 거리를 뜻하므로 이것을 최소화한다면 모델이 더 좋은 성늘을 낸다는 것을 뜻한다.

선형 회귀 모델을 예로 들면 선형 회귀에서는 주로 평균 제곱 오차(Mean squared error)를 비용 함수로 사용한다.

![image-20220430124028731](../../images/2022-05-04-확률적 경사 하강법(혼공머신 4-2)/image-20220430124028731.png)

선형회귀에서  각각의 y값(Weight값)을 
$$
y_1,y_2,...,y_i
$$
라 하고 이 직선을 
$$
\hat{y}=W+bx_i
$$
라고 한다면 비용함수는 예측값과 실제 차이에 대한 평균값으로 구하기 때문에
$$
cost(W,b)= \frac{1}{m} \sum_{i=1}^{m}(W+bx_i-y_i)^2
$$
로 볼 수 있다.
$$
cost(W,b)=MSE=E 라고 하면
\\
E= \frac{1}{m} \sum_{i=1}^{m}(W+bx_i-y_i)^2
\\
\frac{\part E}{\part w}=\frac{\part}{\part w}\sum_{i=1}^{m}(W+bx_i-y_i)^2
\\
gradient=\frac{\part E}{\part w}
$$
미분하면 기울기값을 구할 수 있다. 

# 경사 하강법



# 참고

https://en.wikipedia.org/wiki/Delta_rule