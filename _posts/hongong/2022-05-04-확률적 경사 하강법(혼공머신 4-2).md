# 경사 하강법

여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘으로 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정한다.

## 비용 함수

비용 함수는 선형 모델의 예측과 훈련 데이터 사이의 거리를 뜻하므로 이것을 최소화한다면 모델이 더 좋은 성늘을 낸다는 것을 뜻한다.

선형 회귀 모델을 예로 들면 선형 회귀에서는 주로 평균 제곱 오차(Mean squared error)를 비용 함수로 사용한다.

## 정규 방정식

선형회귀에서  각각의 y값을

$$
y^{1},y^{2},...,y^{i}
$$
라 하고 이 직선을 
$$
\hat{y}=\theta_0 x_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n =\theta \cdot x(x_0=1 가상의 특성)
\\
모델 \ 파라미터:\theta
\\
특성의 \ 개수 : x_1,...,x_n
\\
bias : \theta_0
\\
Weight:\theta_1,...,\theta_n
$$
라 했을 때 MSE는 
$$
MSE(X,h_\theta)=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2
\\
샘플의 개수 :m
$$
로 볼 수 있다.

이 오차가 가장 작아지는 점은 위 함수를 미분했을 때 0이 되는 지점임은 분명하다
$$
MSE=\frac{1}{m}(X\hat{\theta}-y)^2
\\ \\ 
X=\begin{bmatrix}x_0^{(1)}&x_1^{(1)}&...& x_n^{(1)}\\ . \\ . \\ . \\ 
x_0^{(m)}&x_1^{(m)}&...& x_n^{(m)}\end{bmatrix}
\\
m*n행렬 \\
1행 : 1번째 \ 샘플의 \ n개의 \ 특성 \\
m행 : m번째 \ 샘플의 \ n개의 \ 특성 \\
\\
\hat{\theta}=\begin{bmatrix}\theta_0\\ . \\ . \\ . \\ \theta_n\end{bmatrix} \\
1*n행렬\\ \\

y=\begin{bmatrix}y^{1}&...&y^{m}\end{bmatrix}\\
m*1행렬
$$
미분하면
$$
MSE=\frac{1}{m}(X\hat{\theta}-y)^2 \\
0= \frac{2}{m}X^T(X\hat{\theta}-y)\\
Transpose를 \ 해 \ 주어야 \ 곱샘이 \ 가능 \\ \\
X^TX\hat{\theta}=X^Ty\\
\hat{\theta}= (X^TX)^{-1}X^Ty\\
정규방정식
$$
정규방정식을 사용하면 바로 비용함수가 가장 작은 점을 구할 수 있는데 왜 경사 하강법을 사용할까?

이는 특성의 수가 늘수록 행렬 연산에 들어가는 비용이 경사하강법보다 훨씬 증가하기 때문이다.




$$
cost(W,b)=MSE(W) 라고 하면
\\
MSE(W)= \frac{1}{m} \sum_{i=1}^{m}(W+bx_i-y_i)^2
\\
\frac{\part E}{\part w}=\frac{\part}{\part w}\sum_{i=1}^{m}(W+bx_i-y_i)^2
\\
\nabla=\frac{\part E}{\part w}
$$
## 경사 하강법

경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것이다.

![경사하강법1](../../images/2022-05-04-확률적 경사 하강법(혼공머신 4-2)/경사하강법1.png)

비용 함수에서 임의의 초기값으로부터 시작하여 조금씩 비용함수가 감소하는 방향으로 진행하여 최솟값에 수렴할 때 까지 점진적으로 향상시키는 방식이다.


$$
MSE(X,h_\theta)=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2\\
각 \ 파라미터에 \ 대하여 \ 편도함수 \ 구하기 \\
\\
파라미터\theta_j에서의 편도함수:
\frac{\part}{\part\theta_j}MSE(\theta)
=\frac{2}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y{(i)})x_j^{(i)}\\
\\
\nabla_\theta \ MSE(\theta)=
\begin{bmatrix}
\frac{\part}{\part\theta_0}MSE(\theta)\\
\frac{\part}{\part\theta_1}MSE(\theta)
\\.\\.\\.\\
\frac{\part}{\part\theta_n}MSE(\theta)\end{bmatrix}
=\frac{2}{m}X^T(X\theta-y)\\
\\
그래디언트 \ 벡터\nabla_\theta \ MSE(\theta) 는 \ 모든 \ 파라이터의 \ 편도함수값이 \ 들어있다\\
\\
\theta^{next \ step}=
\theta-\eta\nabla_\theta MSE(\theta)
$$


# 참고

https://en.wikipedia.org/wiki/Delta_rule