# 정규방정식과 경사 하강법

## 비용 함수

비용 함수는 선형 모델의 예측과 훈련 데이터 사이의 거리를 뜻하므로 이것을 최소화한다면 모델이 더 좋은 성늘을 낸다는 것을 뜻한다.

선형 회귀 모델을 예로 들면 선형 회귀에서는 주로 평균 제곱 오차(Mean squared error)를 비용 함수로 사용한다.

## 정규 방정식

선형회귀에서  각각의 y값을

$$
y^{1},y^{2},...,y^{i}
$$
라 하고 이 직선을 
$$
\hat{y}=\theta_0 x_0+\theta_1 x_1+\theta_2 x_2+...+\theta_n x_n =\theta \cdot x(x_0=1 가상의 특성)
\\
모델 \ 파라미터:\theta
\\
특성의 \ 개수 : x_1,...,x_n
\\
bias : \theta_0
\\
Weight:\theta_1,...,\theta_n
$$
라 했을 때 MSE는 
$$
MSE(X,h_\theta)=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2
\\
샘플의 개수 :m
$$
로 볼 수 있다.

이 오차가 가장 작아지는 점은 위 함수를 미분했을 때 0이 되는 지점임은 분명하다
$$
MSE=\frac{1}{m}(X\hat{\theta}-y)^2
\\ \\ 
X=\begin{bmatrix}x_0^{(1)}&x_1^{(1)}&...& x_n^{(1)}\\ . \\ . \\ . \\ 
x_0^{(m)}&x_1^{(m)}&...& x_n^{(m)}\end{bmatrix}
\\
m*n행렬 \\
1행 : 1번째 \ 샘플의 \ n개의 \ 특성 \\
m행 : m번째 \ 샘플의 \ n개의 \ 특성 \\
\\
\hat{\theta}=\begin{bmatrix}\theta_0\\ . \\ . \\ . \\ \theta_n\end{bmatrix} \\
1*n행렬\\ \\

y=\begin{bmatrix}y^{1}&...&y^{m}\end{bmatrix}\\
m*1행렬
$$
미분하면
$$
MSE=\frac{1}{m}(X\hat{\theta}-y)^2 \\
0= \frac{2}{m}X^T(X\hat{\theta}-y)\\
Transpose를 \ 해 \ 주어야 \ 곱샘이 \ 가능 \\ \\
X^TX\hat{\theta}=X^Ty\\
\hat{\theta}= (X^TX)^{-1}X^Ty\\
정규방정식
$$
정규방정식을 사용하면 바로 비용함수가 가장 작은 점을 구할 수 있는데 왜 경사 하강법을 사용할까?

이는 특성의 수가 늘수록 행렬 연산에 들어가는 비용이 경사하강법보다 훨씬 증가하기 때문이다.




$$
cost(W,b)=MSE(W) 라고 하면
\\
MSE(W)= \frac{1}{m} \sum_{i=1}^{m}(W+bx_i-y_i)^2
\\
\frac{\part E}{\part w}=\frac{\part}{\part w}\sum_{i=1}^{m}(W+bx_i-y_i)^2
\\
\nabla=\frac{\part E}{\part w}
$$


## 경사 하강법

경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것이다.

![경사하강법1](../../images/2022-05-04-확률적 경사 하강법(혼공머신 4-2)/경사하강법1.png)

비용 함수에서 임의의 초기값으로부터 시작하여 조금씩 비용함수가 감소하는 방향으로 진행하여 최솟값에 수렴할 때 까지 점진적으로 향상시키는 방식이다.

각 스텝에서의 그레디언트값을 구하고 어느 정도만큼 내려갈지 학습률를 정해준다.

이 학습률이 너무 크거나 작다면 문제점이 생길 수 있다.

![경사하강법문제](../../images/2022-05-04-확률적 경사 하강법(혼공머신 4-2)/경사하강법문제.png)

너무 작다면 최솟값까지 너무 오래 걸릴 것이고 너무 크다면 다음 스텝이 오히려 이전보다 더 좋지 못한 결과를 담을 수도 있다.

![경사하강법2](../../images/2022-05-04-확률적 경사 하강법(혼공머신 4-2)/경사하강법2.png)

MSE가 아닌 복잡하고 모델 파라미터가 많은 비용함수의 경우 지역 최솟값으로 빠져버리거나 기울기가 0인 부분에서 멈춰버릴 수도 있다.

![경사하강법3](../../images/2022-05-04-확률적 경사 하강법(혼공머신 4-2)/경사하강법3.png)

또한 특성의 스케일이 맞춰지지 않은 경우 더 시간이 오래 걸린다. 

왼쪽의 그림은 비용이 높은 구간에서는 더 높은 학습률이 적용된 모습이지만 오른쪽은 비효율적인 모습이다.
$$
MSE(X,h_\theta)=\frac{1}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2\\
각 \ 파라미터에 \ 대하여 \ 편도함수 \ 구하기 \\
\\
파라미터\theta_j에서의 편도함수:
\frac{\part}{\part\theta_j}MSE(\theta)
=\frac{2}{m}\sum_{i=1}^m(\theta^Tx^{(i)}-y{(i)})x_j^{(i)}\\
\\
\nabla_\theta \ MSE(\theta)=
\begin{bmatrix}
\frac{\part}{\part\theta_0}MSE(\theta)\\
\frac{\part}{\part\theta_1}MSE(\theta)
\\.\\.\\.\\
\frac{\part}{\part\theta_n}MSE(\theta)\end{bmatrix}
=\frac{2}{m}X^T(X\theta-y)\\
\\
그래디언트 \ 벡터\nabla_\theta \ MSE(\theta) 는 \ 모든 \ 파라이터의 \ 편도함수값이 \ 들어있다\\
\\
\\
경사 \ 하강법의 \ 스텝\\
\theta^{next \ step}=
\theta-\eta\nabla_\theta MSE(\theta)\\
학습률:\eta
$$

## 경사 하강법 종류

배치 경사 하강법 : 매 스텝에서 훈련 데이터 전체를 사용하는 것 훈련 세트가 커질수록 느려진다.

### 확률적 경사 하강법 

무작위 한 개 샘플에 대한 그래디언트를 계산하므로 큰 훈련세트도 빨리 할 수 있지만 훨씬 불안정해진다.

![경사하강법4](../../images/2022-05-04-확률적 경사 하강법(혼공머신 4-2)/경사하강법4.png)

불안정하므로 최솟값에 안착하지 못하지만 또한 지역 최솟값을 건너뛰어버리는 장점도 가지고 있다.

최솟값에 안착하기 위해 학습률을 점차 줄이는 방식을 사용하는데 너무 빨리 줄인다면 지역 최솟값에 갇혀버릴 수 있고 너무 천천히 줄어들면 최솟값을 찾지 못하고 맴돌수도 있다.

훈련 샘플이 IID(Independent and Identically Distributed)되어있어야만 전역 최적값을 향해 진행된다고 보장할 수 있다.

iid

## 참고

https://en.wikipedia.org/wiki/Delta_rule